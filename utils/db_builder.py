#!/usr/bin/env python3
"""
DB Builder - æ„å»ºå‘é‡æ•°æ®åº“
ç”¨é€”ï¼šä¸€æ¬¡æ€§å¤„ç†knowledge_base/ä¸‹çš„PDFæ–‡ä»¶ï¼Œæ„å»ºå‘é‡æ•°æ®åº“
ä½¿ç”¨ï¼špython -m utils.db_builder  # è¿è¡Œä¸€æ¬¡å³å¯
"""

import os
import fitz  # PyMuPDF
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict
import sys

# RAG é…ç½®å¸¸é‡
EMBEDDING_MODEL_NAME = "Qwen/Qwen3-Embedding-0.6B"
FALLBACK_MODEL_NAME = "all-MiniLM-L6-v2"
VECTOR_DB_PATH = "./vector_db"
CHUNK_MIN_LENGTH = 50
BATCH_SIZE = 100


class DBBuilder:
    def __init__(self, cache_dir: str = None):
        print("ğŸš€ åˆå§‹åŒ–æ•°æ®åº“æ„å»ºå™¨...")
        
        # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•åˆ°é¡¹ç›®æ–‡ä»¶å¤¹
        if not cache_dir:
            # é»˜è®¤ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„ embedding_models æ–‡ä»¶å¤¹
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            cache_dir = os.path.join(project_root, "embedding_models")
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        os.environ['HF_HOME'] = cache_dir
        print(f"ğŸ“ ä½¿ç”¨ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
        if device == "cuda":
            print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # ä½¿ç”¨ Qwen3 åµŒå…¥æ¨¡å‹
        try:
            print("ğŸ“¥ åŠ è½½ Qwen3 åµŒå…¥æ¨¡å‹...")
            
            # å°è¯•åŠ è½½ Qwen3-Embedding-0.6B
            try:
                self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, 
                                                         cache_folder=cache_dir,
                                                         device=device)
                print(f"âœ… Qwen3-Embedding-0.6B åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
            except Exception as qwen_error:
                print(f"âš ï¸  Qwen3 æ¨¡å‹åŠ è½½å¤±è´¥: {qwen_error}")
                print("ğŸ”„ å›é€€åˆ°è½»é‡çº§æ¨¡å‹...")
                self.embedding_model = SentenceTransformer(FALLBACK_MODEL_NAME, 
                                                         cache_folder=cache_dir,
                                                         device=device)
                print(f"âœ… å›é€€æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
                
        except Exception as e:
            print(f"âŒ æ‰€æœ‰åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
        
        # åˆå§‹åŒ–ChromaDB
        try:
            print("ğŸ—„ï¸ åˆå§‹åŒ–ChromaDB...")
            self.chroma_client = chromadb.PersistentClient(path=VECTOR_DB_PATH)
            
            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œåˆ é™¤å¹¶é‡æ–°åˆ›å»º
            try:
                self.chroma_client.delete_collection(name="agent_knowledge")
                print("ğŸ”„ åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®åº“é›†åˆ")
            except:
                pass  # é›†åˆä¸å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
            
            # ä½¿ç”¨ç®€å•é›†åˆåˆ›å»ºï¼Œæ‰‹åŠ¨æ§åˆ¶embeddingç”Ÿæˆï¼ˆé¿å…ChromaDBå†…éƒ¨è°ƒç”¨æ­»æœºï¼‰
            self.collection = self.chroma_client.create_collection(
                name="agent_knowledge"
                # ä¸æŒ‡å®šembedding_functionï¼Œæˆ‘ä»¬æ‰‹åŠ¨ç”Ÿæˆå¹¶ä¼ å…¥embedding
            )
            print("âœ… ChromaDBåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ ChromaDBåˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:
        """ä»PDFæå–æ–‡æœ¬å¹¶åˆ†å—"""
        print(f"ğŸ“„ å¤„ç†PDF: {os.path.basename(pdf_path)}")
        
        try:
            doc = fitz.open(pdf_path)
            chunks = []
            
            for page_num in range(doc.page_count):
                page = doc[page_num]
                text = page.get_text()
                
                if not text.strip():
                    continue
                
                # ç®€å•åˆ†å—ï¼šæŒ‰æ®µè½åˆ†å‰²
                paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
                
                for i, paragraph in enumerate(paragraphs):
                    # è¿‡æ»¤å¤ªçŸ­çš„æ–‡æœ¬ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„å†…å®¹
                    if len(paragraph) > CHUNK_MIN_LENGTH:  
                        chunks.append({
                            "text": paragraph,
                            "source": os.path.basename(pdf_path),
                            "page": page_num + 1,
                            "chunk_id": f"{os.path.basename(pdf_path)}_p{page_num+1}_c{i+1}"
                        })
            
            doc.close()
            print(f"  âœ… æå–äº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
            return chunks
            
        except Exception as e:
            print(f"  âŒ å¤„ç†PDFå¤±è´¥: {e}")
            return []
    
    def build_database(self):
        """æ„å»ºå®Œæ•´çš„å‘é‡æ•°æ®åº“"""
        print("ğŸ—ï¸ å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“...")
        
        knowledge_base_dir = "./knowledge_base"
        
        # æŸ¥æ‰¾PDFæ–‡ä»¶
        pdf_files = [f for f in os.listdir(knowledge_base_dir) if f.lower().endswith('.pdf')]
        
        if not pdf_files:
            print("âŒ åœ¨ knowledge_base/ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            print("è¯·å°†PDFæ–‡ä»¶æ”¾å…¥è¯¥ç›®å½•åé‡æ–°è¿è¡Œ")
            return
        
        print(f"ğŸ“š å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶:")
        for pdf_file in pdf_files:
            print(f"  - {pdf_file}")
        
        all_chunks = []
        
        # å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶
        for filename in pdf_files:
            pdf_path = os.path.join(knowledge_base_dir, filename)
            chunks = self.extract_text_from_pdf(pdf_path)
            all_chunks.extend(chunks)
        
        if not all_chunks:
            print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹")
            return
        
        print(f"\nğŸ“Š æ€»å…±éœ€è¦å¤„ç†çš„æ–‡æœ¬å—: {len(all_chunks)}")
        
        # å‡†å¤‡æ•°æ®
        texts = [chunk["text"] for chunk in all_chunks]
        ids = [chunk["chunk_id"] for chunk in all_chunks]
        metadatas = [{"source": chunk["source"], "page": chunk["page"]} 
                    for chunk in all_chunks]
        
        print("ğŸ”„ ç”ŸæˆåµŒå…¥å‘é‡å¹¶å­˜å‚¨åˆ°æ•°æ®åº“...")
        
        try:
            # GPUæ¨¡å¼å¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°
            import torch
            if torch.cuda.is_available():
                batch_size = 16  # GPUæ¨¡å¼ï¼šå¤§æ‰¹é‡
                print("ğŸš€ GPUæ¨¡å¼ï¼šä½¿ç”¨å¤§æ‰¹é‡å¤„ç†")
            else:
                batch_size = 8  # CPUæ¨¡å¼ï¼šå°æ‰¹é‡
                print("ğŸ’» CPUæ¨¡å¼ï¼šä½¿ç”¨å°æ‰¹é‡å¤„ç†")
            
            total_batches = (len(texts) + batch_size - 1) // batch_size
            print(f"ğŸ“Š å°†åˆ† {total_batches} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ {batch_size} ä¸ªæ–‡æœ¬å—")
            
            for i in range(0, len(texts), batch_size):
                batch_num = i // batch_size + 1
                batch_texts = texts[i:i+batch_size]
                batch_ids = ids[i:i+batch_size]
                batch_metadatas = metadatas[i:i+batch_size]
                
                print(f"  ğŸ”„ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹...")
                
                # GPUæ¨¡å¼ä¸‹å¯ä»¥å¯ç”¨è¿›åº¦æ¡å’Œä¼˜åŒ–å‚æ•°
                if torch.cuda.is_available():
                    # GPUæ¨¡å¼ï¼šå¯ç”¨æ··åˆç²¾åº¦å’Œä¼˜åŒ–
                    embeddings = self.embedding_model.encode(
                        batch_texts, 
                        batch_size=32,  # å†…éƒ¨æ‰¹é‡å¤§å°
                        show_progress_bar=False,  # é¿å…è¿‡å¤šè¾“å‡º
                        convert_to_numpy=True,
                        normalize_embeddings=True  # æ ‡å‡†åŒ–å‘é‡
                    )
                else:
                    # CPUæ¨¡å¼ï¼šä¿å®ˆå‚æ•°
                    embeddings = self.embedding_model.encode(
                        batch_texts,
                        batch_size=8,
                        show_progress_bar=False,
                        convert_to_numpy=True
                    )
                
                # å­˜å‚¨åˆ°ChromaDB
                self.collection.add(
                    documents=batch_texts,
                    ids=batch_ids,
                    metadatas=batch_metadatas,
                    embeddings=embeddings.tolist()
                )
                
                print(f"  âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ ({min(i+batch_size, len(texts))}/{len(texts)})")
                
                # GPUæ¨¡å¼ä¸‹æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    print(f"     ğŸ’¾ GPUå†…å­˜: {memory_used:.2f}GB / {memory_total:.1f}GB")
            
            print(f"\nğŸ‰ æ•°æ®åº“æ„å»ºå®Œæˆ!")
            print(f"âœ… æˆåŠŸæ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
            print(f"ğŸ’¾ æ•°æ®åº“ä¿å­˜ä½ç½®: ./vector_db")
            print(f"ğŸ“‹ é›†åˆåç§°: agent_knowledge")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
            for pdf_file in pdf_files:
                file_chunks = [c for c in all_chunks if c["source"] == pdf_file]
                print(f"  - {pdf_file}: {len(file_chunks)} ä¸ªæ–‡æœ¬å—")
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“æ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”§ Agent Knowledge Base Builder")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†è‡ªå®šä¹‰ç¼“å­˜ç›®å½•
    import argparse
    parser = argparse.ArgumentParser(description="æ„å»ºAgentçŸ¥è¯†åº“")
    parser.add_argument("--cache-dir", type=str, 
                       help="æŒ‡å®šæ¨¡å‹ç¼“å­˜ç›®å½• (é»˜è®¤: ./embedding_models)")
    args = parser.parse_args()
    
    builder = DBBuilder(cache_dir=args.cache_dir)
    builder.build_database()
    
    print("\n" + "=" * 60)
    print("âœ¨ æ„å»ºå®Œæˆ! ç°åœ¨å¯ä»¥ä½¿ç”¨RAGæ£€ç´¢åŠŸèƒ½äº†")
    print("ğŸ’¡ ä¸‹ä¸€æ­¥: åœ¨plannerä¸­ä½¿ç”¨çŸ¥è¯†åº“å¢å¼ºä»»åŠ¡è§„åˆ’")
    print("=" * 60)


if __name__ == "__main__":
    main()
